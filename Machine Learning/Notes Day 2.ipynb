{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes Day 2\n",
    "Online resources\n",
    "* The elements of statistical learning 2010\n",
    "* Machine Learning in Materials Science - Reviews in Computational Chemistry volume 29:186-273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Diagrams\n",
    "#### Using Al to ID structural phase maps\n",
    "Unsupervised learning is used for clustering. It finds groups of similar things and clusters them\n",
    "* Can't be checked with cross-validation\n",
    "\n",
    "Supervised learning is used for curve fitting or classification\n",
    "* Can be checked with cross-validation\n",
    "\n",
    "## Unsupervies Learning\n",
    "Use Support vector machines to help make a rule for classifying data into clusters\n",
    "\n",
    "Big Check - \"Normalizing\" data can mess up unsupervised learnign algorithms because it brings the noise up to the signal because we are normalizing in variable space. so mean of all x1 position of the data and same with variance rather than mean across datasets\n",
    "\n",
    "# Dissimilarity Measures -aka kernels\n",
    "Defines how to compare data points \n",
    " * look up all the different invariance options and find the best one for your data\n",
    " \n",
    "Determines how different samples are -comparing diffraction samples How different are they?\n",
    "\n",
    "Most important part is determining how different samples are. \n",
    "\n",
    "Very specific for each \"domain\" or each topic x-ray diffraction different than other data sets\n",
    "\n",
    "Hardest part is to define a dissimilarity measure \n",
    "\n",
    "Common dissimilarity measures\n",
    "* Euclidian distance (normal distance between) aka (L2) in 2 D Distance=sqrt(a^2+b^2)\n",
    "* Taxi-cab / City-block distance (1Right 4Up) aka (L1) in 2 D Distance=a+b\n",
    "* p-norm normalized Euclidean and Taxi-cab (Works well for up to 3D higher dimensional need something else)\n",
    "\n",
    "For X-ray data - list intensity in linear array for image take 2D intensity and convert to 1D array\n",
    "These meansioned measures don't account for many issues\n",
    "\n",
    "Might want a scale invariant (scale of peaks don't matter location matters)\n",
    "Might want a shift invariant (shift between scans (not peaks) doesn't matter just scale)\n",
    "\n",
    "### Scale Invariance\n",
    "* p-norm\n",
    "\n",
    "### Translation Invariance\n",
    "* Cross-bin measure\n",
    "* Use Matrix of band of 1's to allow 1 bin difference that won't matter\n",
    "\n",
    "Look into dynamic time warping\n",
    "Also Earth Mover's Distance -  how much energy would it take to make the bins the same shift piles of dirt\n",
    "\n",
    "Different Invariances are needed for different data sets. Above 3D try all the dissimilarity options and weigh that against computational time\n",
    "\n",
    "If data is unorganized after the dissimilarity you need to reorganize to cluster it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Unmixing\n",
    "### Assuming Linear mixing\n",
    "\n",
    "Signal is made up of Y=Bx+e for multiple x e.g. x=sin, cos, tan, 3x+5\n",
    "\n",
    "Assume the data is composed of a linear mixture of underlying spectra\n",
    "\n",
    "E.G. a diffraction pattern is actually a linear mixture of impulse gaussian functions, 1 for each peak and then add \n",
    "them linearly. Each impulse gaussing function would be a constituent components\n",
    "\n",
    "Use a linear regression of least squares to minimize error of adding these to match data.\n",
    "\n",
    "Regression works by initializing x then solve y, then use y to solve for x and iterate until error is minimized\n",
    "\n",
    "Many possible solutions but you could constain solutions\n",
    "\n",
    "Set constraints - most things you can use non-negative results since oyu can't have negative intensitites\n",
    "\n",
    "Caveat simple case can't handle shifts\n",
    "\n",
    "## NMF non-negative Matrix Factorization - NMF\n",
    "Need to choose number of decompositions\n",
    "\n",
    "No 2 solutions will be the same\n",
    "\n",
    "Look at the result of the factorization to determine how many number of factorizations are needed. For diffraction assume you need 1 factor/decomposition element for each peak. That may be redundant so you can reduce. \n",
    "\n",
    "Can find the optimum number using another method\n",
    "\n",
    "### Minimize Endmember Volume\n",
    "\n",
    "Constrains possible constituents/factors/decompositions such that they are close to the data\n",
    "\n",
    "These constraints are applied to the linear function y=Bx+e \n",
    "\n",
    "## Latent Variable Analysis -LVA\n",
    "\n",
    "### Data Visualization\n",
    "\n",
    "Fit 3D data with a spline sheet and then flatten it. Now 3D->2D\n",
    "\n",
    "Show data in dissimilarity space (Distance between 2 data points 1-d 3 data points -> 2D\n",
    "\n",
    "### Principal Component Analysis-PCA\n",
    "Identifies vector of greatest variance that is 1 principal component then looks orthogonal to get the next and so on\n",
    "\n",
    "Step 1 \n",
    "\n",
    "1) Normalize data by variable not data set (subtract mean)\n",
    "\n",
    "2) find unit vector to maximize projected variance\n",
    "* project onto u1 is Xu1T Uses Covariance Matrix\n",
    "* Use eigen analysis on Covariance Matrix \n",
    "* By not using all eigen values we smooth the data\n",
    "\n",
    "PCA can accidentally hide separation between datasets when visualizing lower dimensional space\n",
    "\n",
    "Can also be used to find outliers\n",
    "\n",
    "Compare PCA and NMF- use constraints on which to use\n",
    "\n",
    "### Multidimensional Data Scaling - MDS\n",
    "Takes Dissimilarity matrix and tries to flatten down to 2D space that spreads data out into clusters\n",
    " \n",
    "#### Aside Feature Engineering = How you present your data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline\n",
    "* What are you trying to learn\n",
    "* Preprocessing\n",
    "* Clean up data\n",
    "* PCA LVA\n",
    "* Dissimilarity Measures\n",
    "* Machine Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Use Dissimilarity measure compare group 1 with group1 and group 2 with group1 and group 1 with group 2 and group 2 with group 2\n",
    "\n",
    "This results in a a dissimilarty matrix that looks cross diagonal\n",
    "\n",
    "How many clusters do you choose? Heuristic techniques get to that eventually\n",
    "\n",
    "How to optimize the dissimilarity matrix to make it perfectly cross diagonal?\n",
    "\n",
    "## K-Means Algorythm\n",
    "Number of k is known \n",
    "\n",
    "pick random point and change cluster point so that self dissimliarity is minimized\n",
    "\n",
    "repeat until optimized\n",
    "\n",
    "Doesn't work well unless data is well separated\n",
    "\n",
    "Works only on lower dimensional dataset\n",
    "\n",
    "Doesn't work on \n",
    "* different densities\n",
    "* non spherical sets\n",
    "* different sizes\n",
    "\n",
    "## Generalization of K-Means -- K-Medoids\n",
    "Only know distances between points - only have dissimilarity measure results\n",
    "\n",
    "Moves center to different data points and minimizes distance to all other points in cluster\n",
    "\n",
    "## How to pick the number of clusters\n",
    "Use k-means\n",
    "\n",
    "minimize within cluster scatter vs number of clusters \n",
    "\n",
    "Find the elbow in the graph\n",
    "* after elbow the rate of decrease becomes small and constant\n",
    "\n",
    "Compare with uniform data\n",
    "\n",
    "Maximize Gap difference between uniform data and the real data\n",
    "\n",
    "## Hierarchical cluster analysis -HCA\n",
    "Searches over all k's \n",
    "Need dissimilarity matrix and dissimilarity between clusters\n",
    "* Distance between center of each cluster\n",
    "* Distance between furthest points\n",
    "* Distance between closest points\n",
    "\n",
    "Visualized using dendogram\n",
    "\n",
    "Merge data points 1 by one until you merge clusters by least dissimilarity and continue until you have 1 big cluster with all the data in it.\n",
    "## Mixture Modle: Gaussian\n",
    "Assumes data is drawn from a set of k Probability density functions specifically gaussian\n",
    "\n",
    "Each pdf is associated with a cluster\n",
    "\n",
    "Computes responsibility probably for point in each pdf\n",
    "\n",
    "Need to set up other parameters like mean std probabilities of each pdf repeat until converged\n",
    "\n",
    "## Spectral Clustering\n",
    "Useful for difficult clustering - Approximation for the best solutions\n",
    "\n",
    "Connect points into a graph. Cut the data into 2. \n",
    "\n",
    "Need Similarity measure. Make Dissimilarity measure and plug it into a similarity function to get the similarity \n",
    "measure. The lines that connect all points (some k nearest neighbors) has a similarity function (weight). Minimize the amount of energy for each cut. Minimium weight of cuts - can cut off outliers - bad cut\n",
    "\n",
    "Normalize by volume of the cuts regions. NP-Hard problem. . . . \n",
    "\n",
    "Form a Graph Laplacian- Related to a random walk - ant is on data point. likelyhood of walkign to next data point is the similarity measure. probability of going to 3 is the similiarity measure between those point divided by the sum of the similarity measure of the ant going to any of the other k nearest neighbors. \n",
    "\n",
    "Very useful for polychrystalline results\n",
    "\n",
    "For more see\n",
    "* A Tutorial on Spectral clustering by Ulrike von Luxborg 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
